{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTjHkHstI8Fy1dHzoUO8C+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasudaya/Boundary-Unlearning/blob/master/unlearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YOebdJfyawrc"
      },
      "outputs": [],
      "source": [
        "# # Step 1: Install and import necessary libraries\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# from torchvision import datasets, models\n",
        "# import time\n",
        "\n",
        "# # Step 2: Set device configuration (GPU if available)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Step 3: Download and preprocess the CIFAR-100 dataset\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomCrop(32, padding=4),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))\n",
        "# ])\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))\n",
        "# ])\n",
        "\n",
        "# # Download the CIFAR-100 dataset\n",
        "# train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "# test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# # Step 4: Create DataLoader for batching and shuffling\n",
        "# batch_size = 128\n",
        "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Step 5: Load a pre-trained ResNet model and modify it for CIFAR-100\n",
        "# model = models.resnet18(pretrained=True)  # You can also use resnet34 or resnet50\n",
        "# num_ftrs = model.fc.in_features\n",
        "# model.fc = nn.Linear(num_ftrs, 100)  # Modify the final layer to match CIFAR-100 classes\n",
        "# model = model.to(device)\n",
        "\n",
        "# # Step 6: Define loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Step 7: Train the model\n",
        "# def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "#     model.train()\n",
        "#     for epoch in range(num_epochs):\n",
        "#         running_loss = 0.0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         for inputs, labels in train_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "#             _, predicted = outputs.max(1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "#         end_time = time.time()\n",
        "#         epoch_loss = running_loss / len(train_loader)\n",
        "#         accuracy = 100. * correct / total\n",
        "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, Time: {end_time - start_time:.2f}s\")\n",
        "\n",
        "# # Step 8: Train the model for a specific number of epochs\n",
        "# train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# # Step 9: Evaluate the model on the test set\n",
        "# def evaluate_model(model, test_loader):\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in test_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             _, predicted = outputs.max(1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "#     accuracy = 100. * correct / total\n",
        "#     print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# evaluate_model(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import copy\n",
        "import numpy as np\n",
        "from torchvision.models import resnet18"
      ],
      "metadata": {
        "id": "qV8_IK2BeQ26"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split trainset into retain and forget subsets\n",
        "    train_retain_size = int(len(trainset) * 0.9)\n",
        "    train_forget_size = len(trainset) - train_retain_size\n",
        "\n",
        "    retain_dataset, forget_dataset = random_split(trainset, [train_retain_size, train_forget_size])\n",
        "\n",
        "    retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return retain_loader, forget_loader, test_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "NYI-y6VNbej8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet model setup\n",
        "def get_model():\n",
        "    model = resnet18(num_classes=100)\n",
        "    return model\n",
        "\n",
        "def likelihood(score, mean, var):\n",
        "    nll = -(((score - mean)**2) / (2 * (var ** 2))) - 0.5 * torch.log(var ** 2) - 0.5 * torch.log(4 * torch.acos(torch.zeros(1)))\n",
        "    return torch.exp(nll)\n",
        "\n"
      ],
      "metadata": {
        "id": "-v6IVPSuel5Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_salun_mask(model, device, forget_loader, threshold=0.1):\n",
        "    mask = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        mask[name] = 0\n",
        "\n",
        "    model.train()\n",
        "    for data, target in forget_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target, reduction='sum')\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    mask[name] += param.grad.data\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for name in mask:\n",
        "            mask[name] = torch.abs_(mask[name])\n",
        "\n",
        "    sorted_dict_positions = {}\n",
        "    hard_dict = {}\n",
        "\n",
        "    # Concatenate all tensors into a single tensor\n",
        "    all_elements = -torch.cat([tensor.flatten() for tensor in mask.values()])\n",
        "    threshold_index = int(len(all_elements) * threshold)\n",
        "\n",
        "    positions = torch.argsort(all_elements)\n",
        "    ranks = torch.argsort(positions)\n",
        "\n",
        "    start_index = 0\n",
        "    for key, tensor in mask.items():\n",
        "        num_elements = tensor.numel()\n",
        "        tensor_ranks = ranks[start_index : start_index + num_elements]\n",
        "        sorted_positions = tensor_ranks.reshape(tensor.shape)\n",
        "        sorted_dict_positions[key] = sorted_positions\n",
        "\n",
        "        threshold_tensor = torch.zeros_like(tensor_ranks)\n",
        "        threshold_tensor[tensor_ranks < threshold_index] = 1\n",
        "        threshold_tensor = threshold_tensor.reshape(tensor.shape)\n",
        "        hard_dict[key] = threshold_tensor\n",
        "        start_index += num_elements\n",
        "\n",
        "    return hard_dict"
      ],
      "metadata": {
        "id": "3hrdGcYFejen"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Train function with SALUN unlearning\n",
        "def salun_train(args, model, device, retain_loader, forget_loader, test_loader, optimizer, epochs, batch_size):\n",
        "    mask = get_salun_mask(model, device, forget_loader, threshold=args.salun_threshold)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for data, target in retain_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply the mask to the gradients\n",
        "            if mask:\n",
        "                for name, param in model.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param.grad *= mask[name]\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(retain_loader)}\")\n",
        "\n",
        "    # Test the model after unlearning\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "# Testing function\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total} %')\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    class Args:\n",
        "        salun_threshold = 0.1\n",
        "        batch_size = 128\n",
        "        lr = 0.1\n",
        "        epochs = 10\n",
        "        momentum = 0.9\n",
        "        weight_decay = 1e-4\n",
        "\n",
        "    args = Args()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    retain_loader, forget_loader, test_loader = load_data(batch_size=args.batch_size)\n",
        "    model = get_model().to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "\n",
        "    # Train the model with SALUN unlearning\n",
        "    salun_train(args, model, device, retain_loader, forget_loader, test_loader, optimizer, epochs=args.epochs, batch_size=args.batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRDT6GR3echr",
        "outputId": "8d43546e-7f1b-4617-ad1b-e47b2d38dc8a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 72968622.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Loss: 2.3724263330752198\n",
            "Epoch [2/10], Loss: 1.6246497085825964\n",
            "Epoch [3/10], Loss: 1.4239645715464244\n",
            "Epoch [4/10], Loss: 1.2978979223831133\n",
            "Epoch [5/10], Loss: 1.175824600695209\n",
            "Epoch [6/10], Loss: 1.0900118858976797\n",
            "Epoch [7/10], Loss: 1.021635575727983\n",
            "Epoch [8/10], Loss: 0.9539693648164923\n",
            "Epoch [9/10], Loss: 0.9179781788790767\n",
            "Epoch [10/10], Loss: 0.8696315175091679\n",
            "Test Accuracy: 66.63 %\n"
          ]
        }
      ]
    }
  ]
}